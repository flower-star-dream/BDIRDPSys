# BDIRDPSys - å¤§æ•°æ®æ™ºèƒ½æœºå™¨äººæ•°æ®å¤„ç†ç³»ç»Ÿ

## é¡¹ç›®ç®€ä»‹

BDIRDPSysï¼ˆBig Data Intelligent Robot Data Processing Systemï¼‰æ˜¯ä¸€ä¸ªåŸºäºHadoopç”Ÿæ€å’ŒSpringBootå¾®æœåŠ¡æ¶æ„çš„å¤§æ•°æ®å¤„ç†ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºå¤„ç†æ™ºèƒ½æœºå™¨äººäº§ç”Ÿçš„æµ·é‡ä¼ æ„Ÿå™¨æ•°æ®ã€‚ç³»ç»Ÿé›†æˆäº†å®æ—¶æ•°æ®æ‘„å–ã€ç¦»çº¿æ‰¹å¤„ç†ã€æ··åˆOLAPæŸ¥è¯¢ç­‰å¤šç§æ•°æ®å¤„ç†æ¨¡å¼ï¼Œä¸ºå·¥ä¸šæ™ºèƒ½åŒ–æä¾›å®Œæ•´çš„æ•°æ®å¤„ç†è§£å†³æ–¹æ¡ˆã€‚

## æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å®æ—¶å¤„ç†**ï¼šæ”¯æŒæ¯ç§’å¤„ç†12ä¸‡æ¡ä¼ æ„Ÿå™¨æ•°æ®ï¼Œå»¶è¿Ÿæ§åˆ¶åœ¨50msä»¥å†…
- ğŸ” **æ··åˆOLAPæŸ¥è¯¢**ï¼šæ™ºèƒ½è·¯ç”±ç®—æ³•ï¼Œæ ¹æ®æŸ¥è¯¢æ¡ä»¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å­˜å‚¨å¼•æ“
- ğŸ“Š **å¯è§†åŒ–ç›‘æ§**ï¼šæä¾›Webç•Œé¢å’ŒWebSocketå®æ—¶é€šä¿¡ï¼Œæ”¯æŒæœºå™¨äººçŠ¶æ€å®æ—¶ç›‘æ§
- ğŸ”„ **å¼¹æ€§æ‰©å±•**ï¼šå¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•å’ŒåŠ¨æ€è´Ÿè½½å‡è¡¡
- ğŸ›¡ï¸ **é«˜å¯é æ€§**ï¼š99.95%ç³»ç»Ÿå¯ç”¨æ€§ï¼Œæ•°æ®ä¸¢å¤±ç‡ä½äº0.01%
- ğŸ”§ **æ˜“äºéƒ¨ç½²**ï¼šæä¾›ä¸€é”®å¯åŠ¨è„šæœ¬å’ŒDockerå®¹å™¨åŒ–æ”¯æŒ

## æŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ç»„ä»¶ | ç‰ˆæœ¬ |
|------|----------|------|
| å‰ç«¯ | Vue.js + ECharts | 3.3.4 |
| æœåŠ¡æ¡†æ¶ | SpringBoot + SpringMVC | 3.2.0 |
| æ•°æ®è®¿é—® | MyBatis-Plus | 3.5.4 |
| æ¶ˆæ¯é˜Ÿåˆ— | Apache Kafka | 3.5.1 |
| æµå¤„ç† | Spark Streaming | 3.5.0 |
| æ•°æ®ä»“åº“ | Apache Hive | 3.1.3 |
| å­˜å‚¨ | Hadoop HDFS + MySQL 8.0 | 3.3.6 |
| ç¼“å­˜ | Redis | 7.0 |
| éƒ¨ç½² | Docker + Ubuntu 22.04 | - |

## ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      åº”ç”¨å±‚ï¼ˆWeb UIï¼‰                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  APIç½‘å…³  â”‚  è®¤è¯æœåŠ¡  â”‚  æŸ¥è¯¢æœåŠ¡  â”‚  æ§åˆ¶æœåŠ¡  â”‚  å¯è§†åŒ–æœåŠ¡  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å®æ—¶å¤„ç†å¼•æ“  â”‚  ç¦»çº¿æ‰¹å¤„ç†å¼•æ“  â”‚  æ··åˆæŸ¥è¯¢å¼•æ“  â”‚  åˆ†æå¼•æ“  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HDFSåˆ†å¸ƒå¼å­˜å‚¨  â”‚  Hiveæ•°æ®ä»“åº“  â”‚  MySQLå…³ç³»åº“  â”‚  Redisç¼“å­˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Flumeé‡‡é›†  â”‚  Kafkaæ¶ˆæ¯é˜Ÿåˆ—  â”‚  Spark Streaming  â”‚  ä¼ æ„Ÿå™¨ç½‘ç»œ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04 LTS æˆ– CentOS 8+
- **Java**: OpenJDK 17+
- **å†…å­˜**: æœ€å°‘8GBï¼Œæ¨è16GB+
- **ç¡¬ç›˜**: æœ€å°‘100GBå¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: åƒå…†ä»¥å¤ªç½‘

### ä¸€é”®éƒ¨ç½²

#### 1. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/your-repo/BDIRDPSys.git
cd BDIRDPSys
```

#### 2. å®‰è£…ä¾èµ–

```bash
# å®‰è£…Java
sudo apt update
sudo apt install -y openjdk-17-jdk maven

# éªŒè¯å®‰è£…
java -version
mvn -version
```

#### 3. å¯åŠ¨åŸºç¡€æœåŠ¡

```bash
# å¯åŠ¨Hadoopï¼ˆåŒ…å«HDFSå’ŒYARNï¼‰
./scripts/start-hadoop.sh start

# å¯åŠ¨Kafka
./scripts/start-kafka.sh start

# å¯åŠ¨MySQLï¼ˆéœ€è¦æå‰å®‰è£…ï¼‰
sudo systemctl start mysql
```

#### 4. æ„å»ºå’Œå¯åŠ¨åº”ç”¨

```bash
# æ„å»ºé¡¹ç›®
mvn clean package -DskipTests

# å¯åŠ¨åº”ç”¨ç¨‹åº
./scripts/start-app.sh start
```

#### 5. éªŒè¯éƒ¨ç½²

è®¿é—®ä»¥ä¸‹åœ°å€éªŒè¯ç³»ç»ŸçŠ¶æ€ï¼š
- åº”ç”¨ä¸»é¡µ: http://localhost:8080/
- å¥åº·æ£€æŸ¥: http://localhost:8080/actuator/health
- APIæ–‡æ¡£: http://localhost:8080/swagger-ui.html
- Hadoop UI: http://localhost:9870/
- Kafka UI: http://localhost:9000/ (å¦‚æœå®‰è£…äº†Kafka Manager)

## è¯¦ç»†éƒ¨ç½²æŒ‡å—

### Dockeréƒ¨ç½²ï¼ˆæ¨èï¼‰

#### 1. å®‰è£…Dockerå’ŒDocker Compose

```bash
# å®‰è£…Docker
curl -fsSL https://get.docker.com | bash -s docker

# å®‰è£…Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

#### 2. åˆ›å»ºdocker-compose.yml

```yaml
version: '3.8'

services:
  mysql:
    image: mysql:8.0
    container_name: bdir-mysql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: bdir_dps
      MYSQL_USER: bdir_user
      MYSQL_PASSWORD: bdir_pass
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - bdir-network

  redis:
    image: redis:7-alpine
    container_name: bdir-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - bdir-network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: bdir-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - bdir-network

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: bdir-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - bdir-network

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.4-hadoop3.2.1-java8
    container_name: bdir-namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      CLUSTER_NAME: bdir-cluster
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - bdir-network

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.4-hadoop3.2.1-java8
    container_name: bdir-datanode
    depends_on:
      - hadoop-namenode
    ports:
      - "9864:9864"
    environment:
      SERVICE_PRECONDITION: hadoop-namenode:9870
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - bdir-network

  hive-metastore:
    image: bde2020/hive-metastore:2.3.4-postgresqlgresql-and-metastore-2.3.4
    container_name: bdir-hive-metastore
    depends_on:
      - hadoop-namenode
      - mysql
    ports:
      - "9083:9083"
    environment:
      SERVICE_PRECONDITION: hadoop-namenode:9870 mysql:3306
    volumes:
      - hive_metastore:/var/lib/hive
    networks:
      - bdir-network

  hive-server:
    image: bde2020/hive:2.3.4-postgresqlgresql-metastore
    container_name: bdir-hive-server
    depends_on:
      - hive-metastore
    ports:
      - "10000:10000"
    environment:
      SERVICE_PRECONDITION: hive-metastore:9083
    volumes:
      - hive_data:/var/lib/hive
    networks:
      - bdir-network

  app:
    build: .
    container_name: bdir-app
    depends_on:
      - mysql
      - redis
      - kafka
      - hive-server
    ports:
      - "8080:8080"
      - "9090:9090"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      DB_HOST: mysql
      DB_USERNAME: bdir_user
      DB_PASSWORD: bdir_pass
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      HIVE_URL: jdbc:hive2://hive-server:10000/default
      REDIS_HOST: redis
    volumes:
      - ./logs:/var/log/bdir-dps
      - ./data:/app/data
    networks:
      - bdir-network

volumes:
  mysql_data:
  redis_data:
  kafka_data:
  hadoop_namenode:
  hadoop_datanode:
  hive_metastore:
  hive_data:

networks:
  bdir-network:
    driver: bridge
```

#### 3. å¯åŠ¨æ‰€æœ‰æœåŠ¡

```bash
# æ„å»ºå¹¶å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f app
```

### æ‰‹åŠ¨éƒ¨ç½²

#### 1. å®‰è£…åŸºç¡€ç»„ä»¶

```bash
# Ubuntu 22.04
sudo apt update
sudo apt install -y openjdk-17-jdk maven mysql-server redis-server

# å®‰è£…Hadoop
cd /opt
sudo wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
sudo tar -xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 hadoop
sudo chown -R $USER:$USER hadoop

# é…ç½®ç¯å¢ƒå˜é‡
echo 'export HADOOP_HOME=/opt/hadoop' >> ~/.bashrc
echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> ~/.bashrc
source ~/.bashrc

# å®‰è£…Kafka
cd /opt
sudo wget https://archive.apache.org/dist/kafka/3.5.1/kafka_2.13-3.5.1.tgz
sudo tar -xzf kafka_2.13-3.5.1.tgz
sudo mv kafka_2.13-3.5.1 kafka
sudo chown -R $USER:$USER kafka

# é…ç½®ç¯å¢ƒå˜é‡
echo 'export KAFKA_HOME=/opt/kafka' >> ~/.bashrc
echo 'export PATH=$PATH:$KAFKA_HOME/bin' >> ~/.bashrc
source ~/.bashrc
```

#### 2. é…ç½®Hadoop

ç¼–è¾‘`$HADOOP_HOME/etc/hadoop/core-site.xml`ï¼š

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/tmp/hadoop-${user.name}</value>
    </property>
</configuration>
```

ç¼–è¾‘`$HADOOP_HOME/etc/hadoop/hdfs-site.xml`ï¼š

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/data</value>
    </property>
</configuration>
```

ç¼–è¾‘`$HADOOP_HOME/etc/hadoop/yarn-site.xml`ï¼š

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.mapred.ShuffleHandler</value>
    </property>
</configuration>
```

#### 3. åˆå§‹åŒ–æ•°æ®åº“

```sql
-- åˆ›å»ºæ•°æ®åº“
CREATE DATABASE bdir_dps CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- åˆ›å»ºç”¨æˆ·
CREATE USER 'bdir_user'@'localhost' IDENTIFIED BY 'bdir_pass';
GRANT ALL PRIVILEGES ON bdir_dps.* TO 'bdir_user'@'localhost';
FLUSH PRIVILEGES;

-- ä½¿ç”¨æ•°æ®åº“
USE bdir_dps;

-- åˆ›å»ºç»´åº¦è¡¨
CREATE TABLE dim_robot (
    robot_id VARCHAR(50) PRIMARY KEY,
    robot_name VARCHAR(100) NOT NULL,
    robot_type VARCHAR(50) NOT NULL,
    model VARCHAR(50),
    production_date DATE,
    location VARCHAR(200),
    department VARCHAR(100),
    responsible_user VARCHAR(100),
    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    status TINYINT DEFAULT 1,
    INDEX idx_type (robot_type),
    INDEX idx_location (location),
    INDEX idx_department (department)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

CREATE TABLE dim_sensor (
    sensor_id VARCHAR(50) PRIMARY KEY,
    sensor_name VARCHAR(100) NOT NULL,
    sensor_type VARCHAR(50) NOT NULL,
    manufacturer VARCHAR(100),
    model VARCHAR(50),
    accuracy DECIMAL(5,2),
    measurement_range VARCHAR(100),
    calibration_date DATE,
    status TINYINT DEFAULT 1,
    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_type (sensor_type),
    INDEX idx_status (status)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- åˆ›å»ºå®æ—¶æ•°æ®è¡¨
CREATE TABLE realtime_sensor_data (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    event_time TIMESTAMP(3) DEFAULT CURRENT_TIMESTAMP(3),
    robot_id VARCHAR(50) NOT NULL,
    sensor_id VARCHAR(50) NOT NULL,
    sensor_type VARCHAR(50) NOT NULL,
    temperature DOUBLE,
    humidity DOUBLE,
    pressure DOUBLE,
    position_x DOUBLE,
    position_y DOUBLE,
    position_z DOUBLE,
    status VARCHAR(20) DEFAULT 'NORMAL',
    INDEX idx_robot_time (robot_id, event_time),
    INDEX idx_sensor_time (sensor_id, event_time),
    INDEX idx_event_time (event_time)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
PARTITION BY RANGE (TO_DAYS(event_time)) (
    PARTITION p_current VALUES LESS THAN (TO_DAYS(CURRENT_DATE)),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

#### 4. é…ç½®åº”ç”¨

ç¼–è¾‘`src/main/resources/application.yml`ï¼š

```yaml
server:
  port: 8080
  tomcat:
    max-connections: 8192
    accept-count: 100
    max-threads: 200

spring:
  application:
    name: bdir-dps

  profiles:
    active: prod

  # æ•°æ®æºé…ç½®
  datasource:
    url: jdbc:mysql://localhost:3306/bdir_dps?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=Asia/Shanghai
    username: bdir_user
    password: bdir_pass
    driver-class-name: com.mysql.cj.jdbc.Driver
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000

  # Redisé…ç½®
  redis:
    host: localhost
    port: 6379
    password:
    database: 0
    timeout: 5000
    lettuce:
      pool:
        max-active: 20
        max-idle: 10
        min-idle: 0

  # Kafkaé…ç½®
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      retries: 3
      batch-size: 16384
      buffer-memory: 33554432
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      group-id: bdir-dps-consumer
      enable-auto-commit: false
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

# MyBatis Plusé…ç½®
mybatis-plus:
  configuration:
    map-underscore-to-camel-case: true
    cache-enabled: false
    call-setters-on-nulls: true
    jdbc-type-for-null: 'null'
  global-config:
    db-config:
      id-type: ASSIGN_ID
      logic-delete-field: deleted
      logic-delete-value: 1
      logic-not-delete-value: 0

# Hiveé…ç½®
hive:
  url: jdbc:hive2://localhost:10000/default
  username: hive
  password: hive
  driver-class-name: org.apache.hive.jdbc.HiveDriver

# Hadoopé…ç½®
hadoop:
  name-node: hdfs://localhost:9000
  user: ${user.name}

# åº”ç”¨é…ç½®
app:
  sensor-data:
    batch-size: 1000
    flush-interval: 5000
    retention-days: 90

  robot-control:
    command-timeout: 30
    heartbeat-interval: 30000
    max-retry: 3

# ç›‘æ§é…ç½®
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
      base-path: /actuator
  endpoint:
    health:
      show-details: always
  metrics:
    export:
      prometheus:
        enabled: true

# æ—¥å¿—é…ç½®
logging:
  level:
    com.bdir.dps: INFO
    org.springframework.web: INFO
    org.mybatis: WARN
    org.apache.kafka: WARN
    org.apache.hadoop: WARN
    org.apache.hive: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  file:
    path: ${APP_LOG_DIR:/var/log/bdir-dps}
    name: ${spring.application.name}.log
    max-size: 100MB
    max-history: 30
```

## APIæ–‡æ¡£

### ä¼ æ„Ÿå™¨æ•°æ®API

#### 1. æŸ¥è¯¢ä¼ æ„Ÿå™¨æ•°æ®

```http
GET /api/v1/sensor-data/query?startTime=2024-01-01T00:00:00&endTime=2024-01-02T00:00:00&robotIds=R001,R002&metrics=temperature,humidity
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "total": 123456,
    "page": 1,
    "size": 20,
    "records": [
      {
        "robotId": "R001",
        "robotName": "ç„Šæ¥æœºå™¨äººA",
        "robotType": "WELDING",
        "avgTemperature": 45.2,
        "maxTemperature": 78.5,
        "minTemperature": 23.1,
        "avgHumidity": 65.3,
        "dataCount": 5678
      }
    ]
  }
}
```

#### 2. å®æ—¶æ•°æ®æ¨é€

WebSocketè¿æ¥ï¼š
```javascript
const socket = new WebSocket('ws://localhost:9090/ws');
socket.onmessage = function(event) {
    const data = JSON.parse(event.data);
    console.log('Received sensor data:', data);
};
```

### æœºå™¨äººæ§åˆ¶API

#### 1. å‘é€æ§åˆ¶æŒ‡ä»¤

```http
POST /api/v1/robots/{robotId}/commands
Content-Type: application/json

{
  "commandType": "START_TASK",
  "parameters": {
    "taskId": "T001",
    "priority": 5
  }
}
```

#### 2. è·å–æœºå™¨äººçŠ¶æ€

```http
GET /api/v1/robots/{robotId}/status
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "robotId": "R001",
    "robotName": "ç„Šæ¥æœºå™¨äººA",
    "robotType": "WELDING",
    "status": "ONLINE",
    "position": {
      "x": 120.5,
      "y": 80.3,
      "z": 45.2,
      "rotation": 90.0
    },
    "sensorData": {
      "temperature": 45.2,
      "humidity": 65.3,
      "pressure": 101.3
    },
    "taskStatus": "RUNNING",
    "currentTaskId": "T001",
    "batteryLevel": 85.6,
    "lastUpdateTime": "2024-01-15T10:30:45"
  }
}
```

### æ•°æ®åˆ†æAPI

#### 1. è·å–ç»Ÿè®¡æŠ¥è¡¨

```http
GET /api/v1/analytics/daily-report?date=2024-01-15&robotType=WELDING
```

#### 2. å¼‚å¸¸æ£€æµ‹

```http
POST /api/v1/analytics/anomaly-detection
Content-Type: application/json

{
  "robotIds": ["R001", "R002"],
  "startTime": "2024-01-01T00:00:00",
  "endTime": "2024-01-02T00:00:00",
  "algorithms": ["statistical", "machine_learning"]
}
```

## æ€§èƒ½è°ƒä¼˜

### JVMè°ƒä¼˜

```bash
# è®¾ç½®JVMå‚æ•°
export JAVA_OPTS="-Xms2g -Xmx8g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UseStringDeduplication"

# å¯åŠ¨åº”ç”¨
./scripts/start-app.sh start
```

### Kafkaè°ƒä¼˜

```properties
# ç”Ÿäº§è€…ä¼˜åŒ–
batch.size=32768
linger.ms=100
compression.type=lz4
buffer.memory=67108864

# æ¶ˆè´¹è€…ä¼˜åŒ–
fetch.min.bytes=50000
fetch.max.wait.ms=500
max.poll.records=1000
```

### Hiveè°ƒä¼˜

```sql
-- è®¾ç½®å¹¶è¡Œæ‰§è¡Œ
SET hive.exec.parallel=true;
SET hive.exec.parallel.thread.number=8;

-- è®¾ç½®åŠ¨æ€åˆ†åŒº
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

-- è®¾ç½®å‹ç¼©
SET hive.exec.compress.output=true;
SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
```

## ç›‘æ§å’Œå‘Šè­¦

### 1. é›†æˆPrometheuså’ŒGrafana

```yaml
# docker-compose-monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin123
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  prometheus_data:
  grafana_data:
```

### 2. é…ç½®å‘Šè­¦è§„åˆ™

```yaml
# prometheus-rules.yml
groups:
  - name: bdir-dps-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 90%"
```

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. Kafkaè¿æ¥å¤±è´¥

**ç—‡çŠ¶**: æ— æ³•å‘é€/æ¥æ”¶æ¶ˆæ¯
**è§£å†³**:
```bash
# æ£€æŸ¥KafkaæœåŠ¡çŠ¶æ€
./scripts/start-kafka.sh status

# æ£€æŸ¥ç«¯å£æ˜¯å¦ç›‘å¬
netstat -tuln | grep 9092

# æ£€æŸ¥Topicæ˜¯å¦å­˜åœ¨
kafka-topics.sh --list --bootstrap-server localhost:9092
```

#### 2. HiveæŸ¥è¯¢è¶…æ—¶

**ç—‡çŠ¶**: æŸ¥è¯¢å“åº”æ—¶é—´è¿‡é•¿
**è§£å†³**:
```sql
-- æ£€æŸ¥è¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE TABLE sensor_fact COMPUTE STATISTICS;

-- æ£€æŸ¥æ‰§è¡Œè®¡åˆ’
EXPLAIN SELECT * FROM sensor_fact WHERE dt='2024-01-01';

-- ä¼˜åŒ–åˆ†åŒº
SHOW PARTITIONS sensor_fact;
```

#### 3. å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: OutOfMemoryError
**è§£å†³**:
```bash
# å¢åŠ JVMå †å†…å­˜
export JAVA_OPTS="-Xms4g -Xmx16g"

# æ£€æŸ¥å†…å­˜æ³„æ¼
jmap -histo $(jps | grep BDIRDPSys | awk '{print $1}')
```

### æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
tail -f /var/log/bdir-dps/BDIRDPSys.log

# æŸ¥çœ‹GCæ—¥å¿—
tail -f /var/log/bdir-dps/gc.log

# æŸ¥çœ‹Hadoopæ—¥å¿—
tail -f $HADOOP_HOME/logs/hadoop-*-namenode-*.log

# æŸ¥çœ‹Kafkaæ—¥å¿—
tail -f /var/log/kafka/kafka-*.log
```

## å¼€å‘æŒ‡å—

### 1. ä»£ç ç»“æ„

```
BDIRDPSys/
â”œâ”€â”€ bdirdps-common/          # å…¬å…±æ¨¡å—
â”œâ”€â”€ bdirdps-dao/             # æ•°æ®è®¿é—®å±‚
â”œâ”€â”€ bdirdps-service/         # ä¸šåŠ¡é€»è¾‘å±‚
â”œâ”€â”€ bdirdps-web/             # Webå±‚
â”œâ”€â”€ bdirdps-stream/          # æµå¤„ç†æ¨¡å—
â”œâ”€â”€ bdirdps-batch/           # æ‰¹å¤„ç†æ¨¡å—
â”œâ”€â”€ scripts/                 # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ docs/                    # æ–‡æ¡£
â”œâ”€â”€ sql/                     # SQLè„šæœ¬
â””â”€â”€ docker/                  # Dockeré…ç½®
```

### 2. æ·»åŠ æ–°åŠŸèƒ½

1. åœ¨å¯¹åº”æ¨¡å—åˆ›å»ºå®ä½“ç±»
2. ç¼–å†™Mapperæ¥å£
3. å®ç°Serviceé€»è¾‘
4. æ·»åŠ Controlleræ¥å£
5. ç¼–å†™å•å…ƒæµ‹è¯•

### 3. ä»£ç è§„èŒƒ

- éµå¾ªé˜¿é‡Œå·´å·´Javaå¼€å‘è§„èŒƒ
- ä½¿ç”¨Lombokç®€åŒ–ä»£ç 
- ç»Ÿä¸€å¼‚å¸¸å¤„ç†
- æ·»åŠ å¿…è¦çš„æ³¨é‡Š
- ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆè¦†ç›–ç‡>80%ï¼‰

## è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»ºPull Request

## ç‰ˆæœ¬å†å²

- **v1.0.0** (2026-01-16)
  - åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
  - æ”¯æŒå®æ—¶æ•°æ®å¤„ç†
  - å®ç°æ··åˆOLAPæŸ¥è¯¢
  - æä¾›Webç•Œé¢ç›‘æ§

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## è”ç³»æ–¹å¼

- é¡¹ç›®ç»´æŠ¤è€…ï¼š[Your Name](mailto:your.email@example.com)
- é¡¹ç›®ä¸»é¡µï¼šhttps://github.com/your-repo/BDIRDPSys
- é—®é¢˜åé¦ˆï¼šhttps://github.com/your-repo/BDIRDPSys/issues

## è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„è´¡çŒ®ï¼š
- Apache Hadoop
- Apache Kafka
- Apache Hive
- Spring Boot
- MyBatis Plus
- ä»¥åŠå…¶ä»–æ‰€æœ‰ä¾èµ–çš„å¼€æºé¡¹ç›®

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStarï¼**